spring.application.name=llama-text-analyzer
springdoc.show-actuator=true
springdoc.swagger-ui.path=/swagger-ui.html
management.endpoints.web.exposure.include=*
spring.mvc.problemdetails.enabled=true
logging.level.org.springframework.ai=DEBUG
logging.level.org.springframework.web.nodeValue=DEBUG
logging.level.org.springframework.web.servlet=DEBUG
logging.level.web=DEBUG
logging.level.org.springframework.web=TRACE
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.enabled=true
spring.ai.ollama.chat.options.format=json
#https://pkg.go.dev/time#ParseDuration, Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
spring.ai.ollama.chat.options.keep-alive=5m
# The mirostat of the model. Increasing the mirostat will make the model answer more creatively.
spring.ai.ollama.chat.options.mirostat=5
spring.ai.ollama.chat.options.model=phi3
# size of the context window used to generate the next token, Number of contexts to keep in memory
spring.ai.ollama.chat.options.num-ctx=2048
# The temperature of the model. Increasing the temperature will make the model answer more creatively.
spring.ai.ollama.chat.options.temperature=0.5
# A higher value (e.g., 100) will give more diverse answers, while a lower value (e.g., 10) will be more conservative. default 40
spring.ai.ollama.chat.options.top-k=40
# A value closer to 1 will give more diverse answers, while a value closer to 0 will be more conservative. default 0.9
spring.ai.ollama.chat.options.top-p=0.5








